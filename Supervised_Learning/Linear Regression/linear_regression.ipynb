{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8281a1",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "- Here we implement a linear regression method on the Fish weight dataset. We'll be tryiing to predict the weight of the fish based on its various attributes like length and height. \n",
    "\n",
    "- We will be using a linear activation function for simplicity\n",
    "\n",
    "- We will run the model to see how well its able to predict fish weights and use extensive visualization as sanity checks and a rudimentary judgement of the effectiveness. We'll also conduct sensitivity analysis to see how the prediction power differ when we change inputs like the learning rate.\n",
    "\n",
    "- I'm also curious as to what standarization is used for in the context of fitting models, so I will be training models with and without standarization to see how they differ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec78f9",
   "metadata": {},
   "source": [
    "# Define global Variables\n",
    "- Here we will define global variables for convenience and consistency, of which there is only one:\n",
    "    - global_epochs, the number of epochs to be ran for all linear regression models used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining global variables\n",
    "\n",
    "global_epochs = 100   # number of epochs to be run with all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de4537",
   "metadata": {},
   "source": [
    "# 1- Importing necessary libraries and dataset\n",
    "- we also get rid of any entries that contains 0 for any attributes to avoid runing into division by 0 later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from linear_regression_method import SingleNeuron\n",
    "\n",
    "\n",
    "fish_data = pd.read_csv(\"../../data/Fish.csv\")\n",
    "initial_len = len(fish_data)\n",
    "fish_data = fish_data[(fish_data != 0).all(axis=1)]\n",
    "removed = initial_len - len(fish_data)\n",
    "\n",
    "print(f\"Removed {removed} rows with zero in any attribute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24937a",
   "metadata": {},
   "source": [
    "# 2 - Visualize each attribute against weight\n",
    "- Since there are multiple attributes, we'll plot each of them against the independent variable to visually check if they are correlated with weight.\n",
    "\n",
    "- Since linear regression relies on the asumption that the relationship between the independent and dependent variable is linear, we'll also have to confirm visually if this is the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d17e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['Length1', 'Length2', 'Length3', 'Height', 'Width', \"Species\"]\n",
    "\n",
    "for attribute in attributes:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(fish_data['Weight'], fish_data[attribute], alpha=0.7)\n",
    "    plt.title(f'{attribute} vs Weight')\n",
    "    plt.xlabel('Weight')\n",
    "    plt.ylabel(attribute)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279b359",
   "metadata": {},
   "source": [
    "** Remarks **\n",
    "- for length 1-3, width, height, the relationship appears to be logrithmic than linear, which will  throw off our linear regression if left untreated\n",
    "\n",
    "- to address this, we'll transform the depenedent variable to log(weight) and check again to see if we can salvage a linear relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053562b",
   "metadata": {},
   "source": [
    "# 2.1 - Data linearization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e2457",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_data['Log_weight'] = np.log(fish_data['Weight'])\n",
    "\n",
    "attributes = ['Length1', 'Length2', 'Length3', 'Height', 'Width', \"Species\"]\n",
    "for attribute in attributes:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(fish_data['Log_weight'], fish_data[attribute], alpha=0.7)\n",
    "    plt.title(f'{attribute} vs Log_weight')\n",
    "    plt.xlabel('Log_weight')\n",
    "    plt.ylabel(attribute)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed8f4f",
   "metadata": {},
   "source": [
    "** Remarks **\n",
    "\n",
    "- Much better! There is still a bit of curvature left, but it is linear enough for us to proceed with linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a50d11",
   "metadata": {},
   "source": [
    "# 3 - Running linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd19ad",
   "metadata": {},
   "source": [
    "- We'll define the following simple linear activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08398e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43a401",
   "metadata": {},
   "source": [
    "# 3.1 Initiate and train model(Linear activation)\n",
    "- We split the data into training and testing set to avoid overfitting\n",
    "\n",
    "- We train two single neuron model using the linear activation function, one using standarization and one without to see the effect of standarization on the result\n",
    "\n",
    "- For now we assume a learning rate of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6011595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Feature and Target\n",
    "X = fish_data[['Width']].values\n",
    "y = fish_data['Log_weight'].values\n",
    "\n",
    "# --- Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# === 1. With Standardization ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_std = SingleNeuron(activation_function=linear_activation)\n",
    "model_std.train(X_train_scaled, y_train, alpha=0.01, epochs = global_epochs)\n",
    "y_pred_std = model_std.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# === 2. Without Standardization ===\n",
    "model_raw = SingleNeuron(activation_function=linear_activation)\n",
    "model_raw.train(X_train, y_train, alpha=0.01, epochs = global_epochs)\n",
    "y_pred_raw = model_raw.predict(X_test_scaled)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5769fd20",
   "metadata": {},
   "source": [
    "# 4 - Print and interpret output\n",
    "\n",
    "- now that the model is trained, we will have it print out some performance metrics of the final regression. We will be using the Mean Standard Error, Root MSE, Mean Absolute Error, and R^2. This will be done for the standarized model for simplicity\n",
    "\n",
    "- We want to gather information about the accuracy, spread, and how much of the variation is explained by the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aafe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Performance Metrics ===\n",
    "mse_std = mean_squared_error(y_test, y_pred_std)\n",
    "rmse_std = np.sqrt(mse_std)\n",
    "mae_std = mean_absolute_error(y_test, y_pred_std)\n",
    "r2_std = r2_score(y_test, y_pred_std)\n",
    "\n",
    "\n",
    "# === Print Comparison ===\n",
    "print(\"Standardized Model:\")\n",
    "print(f\"  MSE:  {mse_std:.4f}\")\n",
    "print(f\"  RMSE: {rmse_std:.4f}\")\n",
    "print(f\"  MAE:  {mae_std:.4f}\")\n",
    "print(f\"  R²:   {r2_std:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcfbcd5",
   "metadata": {},
   "source": [
    "** Remarks **\n",
    "\n",
    "(the exact numbers might be different as I changed the global epoch number!)\n",
    "\n",
    "- MSE (Mean Squared Error): 0.2482 — This is quite low, indicating that the model's predictions are very close to the actual values on average.\n",
    "\n",
    "- RMSE (Root Mean Squared Error): 0.4982 — This is the square root of the MSE. A low RMSE value also suggests that our predictions are close to the true values, on average.\n",
    "\n",
    "- MAE (Mean Absolute Error): 0.4006 — This is another measure of error, showing that the model is off by an average of about 0.4 units in terms of absolute difference from the true values.\n",
    "\n",
    "- R^2: 0.8720 — Very strong result, meaning that the model explains 87.2% of the variance in the target variable.\n",
    "\n",
    "As we can see, the linear model did a pretty good job at learning to predict the weight of fish based on various attributes. It is both accurate and consistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667c3dc",
   "metadata": {},
   "source": [
    "# 5 - Visualizing the regression\n",
    "- we will visualize the result by plotting the fitted model over test and train data(distinguished by color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e683b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combined Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='lightblue', label='Train Data', alpha=0.6)\n",
    "plt.scatter(X_test, y_test, color='lightgreen', label='Test Data', alpha=0.6)\n",
    "plt.plot(X_test, y_pred_std, color='blue', linewidth=2, label='Regression Line (Standardized)')\n",
    "plt.plot(X_test, y_pred_std, color='orange', linewidth=2, linestyle='--', label='Regression Line (No Standardization)')\n",
    "plt.title('Linear Regression Comparison: Standardized vs. Raw Input')\n",
    "plt.xlabel('Width (cm)')\n",
    "plt.ylabel('log(Weight)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7ab7f",
   "metadata": {},
   "source": [
    "** Remarks **\n",
    "- we confirm our observations of the model's metrics, and it is indeed quite good with the line going through the clusters and following the linear trend. \n",
    "\n",
    "- we also note that the lines for the Standarized model and raw model overlapse, which means the differnce lies not in the resulting model, or the effects are diminished after large number of epochs.\n",
    "\n",
    "- since the main point of standarization is to accelerate the MSE convergence, we'll take a look at exactly that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f7c82",
   "metadata": {},
   "source": [
    "# 6 - Visualizing error over epochs for standarized and raw model\n",
    "- We will plot the errors over opchs for both models to see if standarization had an inpact on the magnitude and slope of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27daeab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_std = model_std.errors_\n",
    "errors_raw = model_raw.errors_\n",
    "\n",
    "# --- Plot Error Comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(errors_std, label=\"With Standardization\", marker='o')\n",
    "plt.plot(errors_raw, label=\"Without Standardization\", marker='s')\n",
    "plt.title(\"Mean Squared Error over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c0545",
   "metadata": {},
   "source": [
    "** Remarks **\n",
    "\n",
    "- clearly, standarization served its purpose, with the error rapidly decreasing following an initial spike at the 1st epoch. This means it is definitely more computationaly efficient to use standarization as it drastically decreases the number of epochs needed to be run to get to an acceptable error level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc17b7",
   "metadata": {},
   "source": [
    "# 7 - Sensitivity analysis of learning rate\n",
    "- we know that the learning rate is one of, if not the most influential factor in machine learning. Therefore, we want to see how the regression result differs when we try different values of learning rates.\n",
    "\n",
    "- here we define an array of different learning rates to try, which are then trained into their own models and plotted against the data. We should be able to see from the plots if the model differed greatly for the different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Possible choices of learning rate\n",
    "alphas = [0.01, 0.05, 0.07, 0.09]\n",
    "\n",
    "# --- Generate a domain of values for plotting the regression line\n",
    "domain = np.linspace(np.min(X_train_scaled) - 0.5, np.max(X_train_scaled) + 0.5, 100)\n",
    "\n",
    "# --- Create the subplot grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# --- Loop over the axes and alpha values\n",
    "for ax, alpha in zip(axs.flat, alphas):\n",
    "    # Train the model with the current learning rate\n",
    "    model = SingleNeuron(activation_function=linear_activation)\n",
    "    model.train(X_train_scaled, y_train, alpha=alpha, epochs = global_epochs)\n",
    "    \n",
    "    # Plot the regression line on the current axis (for the given alpha)\n",
    "    ax.plot(domain, model.predict(domain.reshape(-1, 1)), color='red', linewidth=2)\n",
    "    \n",
    "    # Scatter the training data points on the current axis\n",
    "    ax.scatter(X_train_scaled, y_train, color='lightseagreen', label=\"Train Data\", alpha=0.6)\n",
    "    \n",
    "    # Set the title for the current subplot\n",
    "    ax.set_title(f\"alpha = {alpha}\", fontsize=18)\n",
    "    ax.set_xlabel('Width (cm) (Standardized)')\n",
    "    ax.set_ylabel('Log(Weight)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# --- Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d775f3",
   "metadata": {},
   "source": [
    "** Remarks ** \n",
    "- Somewhat suprisingly, all the models seemed consistent and produced a pretty good linear model, with little differences between them. \n",
    "\n",
    "- I suspect it is due to the conservative set of learning rates I used (alphas = [0.01, 0.05, 0.07, 0.09]), which are all close enough to the original learning rate 0.01. Maybe trying more extreme values would throw the model off?\n",
    "\n",
    "- Or this could also be a result of the large number of epochs used(100/500), so the impact of the learning rate is greatly diminished when viewed across the entire domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35dff7",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "- As we can see from the result, Linear Regression method worked great for this dataset. This makes sense as we know the weight of the fish is obviously dependent on the dimention and specie of the fish.\n",
    "\n",
    "- its also important to note that this example also demonstrates how clearly non-linear data can still be transformed to fit linear regression using mathematical operations. Despite having to transform the dependent variable, the model worked well and preserved its prediction power and consistency. this will be a very valueble lesson for me in the future when I work with any dataset that doesn't immediately appear linear\n",
    "\n",
    "- we also note that the effects of different learning rate was not nearly as noticable as I expected, but this is probably due to the large number of epochs we used - akin to how increasing sample size decreases the variance in a dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
